tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"'")
regex <- "\u201C|\u201D"
foundDQUOTE <- sum(stri_count_regex(tidyText$text,regex))
foundDQUOTE
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"\"")
View(tidyText)
foundCONT <- 0
for(i in seq(1,length(contractions$contraction)))
{
foundCONT <- foundCONT + sum(stri_count_fixed(tidyText$text,contractions$contraction[i]))
tidyText$text <- stri_replace_all_fixed(tidyText$text,contractions$contraction[i],contractions$expanded[i])
}
foundCONT
regex <- "[[:punct:]]"
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
regex <- "\\u00F8"
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
View(tidyText)
$
regex <- "\u2018|\u2019|\u00B4"
foundQUOTE <- sum(stri_count_regex(tidyText$text,regex))
foundQUOTE
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"'")
regex <- "\u201C|\u201D"
foundDQUOTE <- sum(stri_count_regex(tidyText$text,regex))
foundDQUOTE
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"\"")
foundCONT <- 0
for(i in seq(1,length(contractions$contraction)))
{
foundCONT <- foundCONT + sum(stri_count_fixed(tidyText$text,contractions$contraction[i]))
tidyText$text <- stri_replace_all_fixed(tidyText$text,contractions$contraction[i],contractions$expanded[i])
}
foundCONT
View(tidyText)
View(tidyText)
regex <- "[[:punct:]\\^\\$]"
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
regex <- "\\u00F8"
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
View(tidyText)
knitr::opts_chunk$set(cache=TRUE)
library(lubridate)
library(stringi)
library(tidyr)
library(dplyr)
library(ggplot2)
#library(tm)
#library(quanteda)
library(tidytext)
library(tokenizers)
library(qdapDictionaries)
setwd("C:/Users/Luis/Google Drive/Documentos/Coursera/10 Capstone")
Sys.setlocale("LC_TIME", "English")
if(!file.exists("Coursera-SwiftKey.zip"))
{
fileUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download.file(fileUrl,"Coursera-SwiftKey.zip")
}
if(!file.exists("Corpus_raw"))
{
dir.create("Corpus_raw")
unzip("Coursera-SwiftKey.zip",files=c("final/en_US/en_US.twitter.txt","final/en_US/en_US.news.txt","final/en_US/en_US.blogs.txt"),junkpaths=TRUE,exdir="Corpus_raw")
}
if(!file.exists("Corpus_sample"))
{
dir.create("Corpus_sample")
}
if(!file.exists("Corpus_sample/en_US.blogs.txt"))
{
file.create("Corpus_sample/en_US.blogs.txt")
}
if(!file.exists("Corpus_sample/en_US.news.txt"))
{
file.create("Corpus_sample/en_US.news.txt")
}
if(!file.exists("Corpus_sample/en_US.twitter.txt"))
{
file.create("Corpus_sample/en_US.twitter.txt")
}
conRead.blogs <- file("Corpus_raw/en_US.blogs.txt","r")
nLines.blogs <- 0
while((linesRead <- length(readLines(conRead.blogs,10000,skipNul=TRUE))) > 0 )
{
nLines.blogs <- nLines.blogs + linesRead
}
close(conRead.blogs)
conRead.news <- file("Corpus_raw/en_US.news.txt","r")
nLines.news <- 0
while((linesRead <- length(readLines(conRead.news,10000,skipNul=TRUE))) > 0 )
{
nLines.news <- nLines.news + linesRead
}
close(conRead.news)
conRead.twitter <- file("Corpus_raw/en_US.twitter.txt","r")
nLines.twitter <- 0
while((linesRead <- length(readLines(conRead.twitter,10000,skipNul=TRUE))) > 0 )
{
nLines.twitter <- nLines.twitter + linesRead
}
close(conRead.twitter)
nLines.blogs
nLines.news
nLines.twitter
set.seed(1984)
idxVect.blogs <- which(as.logical(rbinom(nLines.blogs,1,.01)))
idxDist.blogs <- idxVect.blogs - c(0,idxVect.blogs[1:length(idxVect.blogs)-1])
idxVect.news <- which(as.logical(rbinom(nLines.news,1,.01)))
idxDist.news <- idxVect.news - c(0,idxVect.news[1:length(idxVect.news)-1])
idxVect.twitter <- which(as.logical(rbinom(nLines.twitter,1,.01)))
idxDist.twitter <- idxVect.twitter - c(0,idxVect.twitter[1:length(idxVect.twitter)-1])
conRead.blogs <- file("Corpus_raw/en_US.blogs.txt","r")
conWrite.blogs <- file("Corpus_sample/en_US.blogs.txt", "w")
for(i in idxDist.blogs)
{
lineRead <- tail(readLines(conRead.blogs,n=i,skipNul=TRUE),1)
writeLines(lineRead,conWrite.blogs)
}
close(conRead.blogs)
close(conWrite.blogs)
conRead.news <- file("Corpus_raw/en_US.news.txt","r")
conWrite.news <- file("Corpus_sample/en_US.news.txt", "w")
for(i in idxDist.news)
{
lineRead <- tail(readLines(conRead.news,n=i,skipNul=TRUE),1)
writeLines(lineRead,conWrite.news)
}
close(conRead.news)
close(conWrite.news)
conRead.twitter <- file("Corpus_raw/en_US.twitter.txt","r")
conWrite.twitter <- file("Corpus_sample/en_US.twitter.txt", "w")
for(i in idxDist.twitter)
{
lineRead <- tail(readLines(conRead.twitter,n=i,skipNul=TRUE),1)
writeLines(lineRead,conWrite.twitter)
}
close(conRead.twitter)
close(conWrite.twitter)
conRead.blogs <- file("Corpus_sample/en_US.blogs.txt","r")
rawText.blogs <- data_frame(fileName="blogs",text=readLines(conRead.blogs,encoding="UTF-8"))
close(conRead.blogs)
conRead.news <- file("Corpus_sample/en_US.news.txt","r")
rawText.news <- data_frame(fileName="news",text=readLines(conRead.news,encoding="UTF-8"))
close(conRead.news)
conRead.twitter <- file("Corpus_sample/en_US.twitter.txt","r")
rawText.twitter <- data_frame(fileName="twitter",text=readLines(conRead.twitter,encoding="UTF-8"))
close(conRead.twitter)
tidyText <- rbind(rawText.blogs,rawText.news,rawText.twitter)
rm(rawText.blogs,rawText.news,rawText.twitter)
regex <- "\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b"
foundEMAILS <- sum(stri_count_regex(tidyText$text,regex))
foundEMAILS
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
regex <- "(\\b(http|HTTP)[\\S]*)|(\\b(www|WWW)\\.[\\S]*)"
foundURLS1 <- sum(stri_count_regex(tidyText$text,regex))
foundURLS1
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
regex <- "[\\S]*\\.(com|org|edu|gov|net|biz|gl|COM|ORG|EDU|GOV|NET|BIZ|GL)(?![a-zA-Z])[/]?[\\S]*"
foundURLS2 <- sum(stri_count_regex(tidyText$text,regex))
foundURLS2
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
regex <- "@\\s"
foundAT <- sum(stri_count_regex(tidyText$text,regex))
foundAT
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"at ")
regex <- "@[\\S]+"
foundUSER <- sum(stri_count_regex(tidyText$text,regex))
foundUSER
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
regex <- "\\.([a-zA-Z]{2,})"
foundDOTERR <- sum(stri_count_regex(tidyText$text,regex))
foundDOTERR
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"\\. $1")
regex <- "\\.[\\.]{1,}"
foundEXDOTS <- sum(stri_count_regex(tidyText$text,regex))
foundEXDOTS
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"\\. ")
regex <- "\u2026"
foundELLIPSIS <- sum(stri_count_regex(tidyText$text,regex))
foundELLIPSIS
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
regex <- ":"
foundCOLON <- sum(stri_count_regex(tidyText$text,regex))
foundCOLON
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"\\. ")
regex <- "(?<!\\w|')([a-zA-Z])\\."
foundABBDOT <- sum(stri_count_regex(tidyText$text,regex))
foundABBDOT
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"$1")
regex <- "[0-9]+"
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"")
tidyText <- unnest_tokens(tidyText,text,text,token="sentences",to_lower = TRUE)
data(contractions)
contractions <- transmute(contractions,contraction=tolower(contraction),expanded=tolower(expanded))
contractions <- rbind(contractions,c("y'all","you all"))
contractions <- rbind(contractions,c("here's","here is"))
contractions <- rbind(contractions,c("haven't","have not"))
regex <- "\u2018|\u2019|\u00B4"
foundQUOTE <- sum(stri_count_regex(tidyText$text,regex))
foundQUOTE
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"'")
regex <- "\u201C|\u201D"
foundDQUOTE <- sum(stri_count_regex(tidyText$text,regex))
foundDQUOTE
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"\"")
foundCONT <- 0
for(i in seq(1,length(contractions$contraction)))
{
foundCONT <- foundCONT + sum(stri_count_fixed(tidyText$text,contractions$contraction[i]))
tidyText$text <- stri_replace_all_fixed(tidyText$text,contractions$contraction[i],contractions$expanded[i])
}
foundCONT
regex <- "'s\\s"
foundPOSS <- sum(stri_count_regex(tidyText$text,regex))
foundPOSS
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
View(tidyText)
regex <- "[[:punct:]\\^\\$]"
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
regex <- "\\u00F8"
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
View(tidyText)
tidyTextWords <- unnest_tokens(tidyText,words,text,token="words")
View(tidyTextWords)
grep("[^a-z]",tidyTextWords$words,value=TRUE)
foreignWords <- tidyTextWords[grep("[^a-z]",tidyTextWords$words),]
View(foreignWords)
foreignWords
library(kable)
library(knitr)
kable(foreignWords)
View(foreignWords)
foreignWords[147,]
foreignWords[147,2]
unlist(foreignWords[147,2])
foreignWords
foreignWords[1:274,]
foreignWords$words
now()
Sys.getlocale()
knitr::opts_chunk$set(cache=TRUE,results='asis')
options(knitr.table.format = "html")
library(lubridate)
library(stringi)
library(tidyr)
library(dplyr)
library(ggplot2)
library(RColorBrewer)
library(tidytext)
library(tokenizers)
library(qdapDictionaries)
library(knitr)
library(kableExtra)
setwd("C:/Users/Luis/Google Drive/Documentos/Coursera/10 Capstone")
#Download file from URL:
if(!file.exists("Coursera-SwiftKey.zip"))
{
fileUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download.file(fileUrl,"Coursera-SwiftKey.zip")
}
#Unzip downloaded file in Corpus_raw dir:
if(!file.exists("Corpus_raw"))
{
dir.create("Corpus_raw")
unzip("Coursera-SwiftKey.zip",files=c("final/en_US/en_US.twitter.txt","final/en_US/en_US.news.txt","final/en_US/en_US.blogs.txt"),junkpaths=TRUE,exdir="Corpus_raw")
}
#Count the number of lines in each original file:
conRead.blogs <- file("Corpus_raw/en_US.blogs.txt","r")
nLines.blogs <- 0
while((linesRead <- length(readLines(conRead.blogs,10000,skipNul=TRUE))) > 0 )
{
nLines.blogs <- nLines.blogs + linesRead
}
close(conRead.blogs)
conRead.news <- file("Corpus_raw/en_US.news.txt","r")
nLines.news <- 0
while((linesRead <- length(readLines(conRead.news,10000,skipNul=TRUE))) > 0 )
{
nLines.news <- nLines.news + linesRead
}
close(conRead.news)
conRead.twitter <- file("Corpus_raw/en_US.twitter.txt","r")
nLines.twitter <- 0
while((linesRead <- length(readLines(conRead.twitter,10000,skipNul=TRUE))) > 0 )
{
nLines.twitter <- nLines.twitter + linesRead
}
close(conRead.twitter)
df <- data_frame(File=c("Blogs","News","Twitter"),Number_of_lines=c(nLines.blogs,nLines.news,nLines.twitter))
kable(df) %>%
kable_styling(bootstrap_options = c("striped", "hover","condensed"),full_width = FALSE)
#Create a vector of line indices to read using a binomial distribution (p=0.01) and calculate the vector of distance between indices:
set.seed(1984)
p <- .01
idxVect.blogs <- which(as.logical(rbinom(nLines.blogs,1,p)))
idxDist.blogs <- idxVect.blogs - c(0,idxVect.blogs[1:length(idxVect.blogs)-1])
idxVect.news <- which(as.logical(rbinom(nLines.news,1,p)))
idxDist.news <- idxVect.news - c(0,idxVect.news[1:length(idxVect.news)-1])
idxVect.twitter <- which(as.logical(rbinom(nLines.twitter,1,p)))
idxDist.twitter <- idxVect.twitter - c(0,idxVect.twitter[1:length(idxVect.twitter)-1])
#Create dir and files if they don't exist:
if(!file.exists("Corpus_sample"))
{
dir.create("Corpus_sample")
}
if(!file.exists("Corpus_sample/en_US.blogs.txt"))
{
file.create("Corpus_sample/en_US.blogs.txt")
}
if(!file.exists("Corpus_sample/en_US.news.txt"))
{
file.create("Corpus_sample/en_US.news.txt")
}
if(!file.exists("Corpus_sample/en_US.twitter.txt"))
{
file.create("Corpus_sample/en_US.twitter.txt")
}
#Read lines using the vector of distance between indices and write lines to new files:
conRead.blogs <- file("Corpus_raw/en_US.blogs.txt","r")
conWrite.blogs <- file("Corpus_sample/en_US.blogs.txt", "w")
for(i in idxDist.blogs)
{
lineRead <- tail(readLines(conRead.blogs,n=i,skipNul=TRUE),1)
writeLines(lineRead,conWrite.blogs)
}
close(conRead.blogs)
close(conWrite.blogs)
conRead.news <- file("Corpus_raw/en_US.news.txt","r")
conWrite.news <- file("Corpus_sample/en_US.news.txt", "w")
for(i in idxDist.news)
{
lineRead <- tail(readLines(conRead.news,n=i,skipNul=TRUE),1)
writeLines(lineRead,conWrite.news)
}
close(conRead.news)
close(conWrite.news)
conRead.twitter <- file("Corpus_raw/en_US.twitter.txt","r")
conWrite.twitter <- file("Corpus_sample/en_US.twitter.txt", "w")
for(i in idxDist.twitter)
{
lineRead <- tail(readLines(conRead.twitter,n=i,skipNul=TRUE),1)
writeLines(lineRead,conWrite.twitter)
}
close(conRead.twitter)
close(conWrite.twitter)
conRead.blogs <- file("Corpus_sample/en_US.blogs.txt","r")
rawText.blogs <- data_frame(fileName="blogs",text=readLines(conRead.blogs,encoding="UTF-8"))
close(conRead.blogs)
conRead.news <- file("Corpus_sample/en_US.news.txt","r")
rawText.news <- data_frame(fileName="news",text=readLines(conRead.news,encoding="UTF-8"))
close(conRead.news)
conRead.twitter <- file("Corpus_sample/en_US.twitter.txt","r")
rawText.twitter <- data_frame(fileName="twitter",text=readLines(conRead.twitter,encoding="UTF-8"))
close(conRead.twitter)
tidyText <- rbind(rawText.blogs,rawText.news,rawText.twitter)
rm(rawText.blogs,rawText.news,rawText.twitter)
#Remove e-mails:
regex <- "\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b"
foundEMAILS <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
#Remove URLs (starting in http or www):
regex <- "((http|HTTP)[\\S]+)|((www|WWW)\\.[\\S]+)"
foundURLS1 <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
#Remove URLs (written without http or www). Most common domain extensions are used:
regex <- "[\\S]*\\.(com|org|edu|gov|net|biz|gl|ly|COM|ORG|EDU|GOV|NET|BIZ|GL|LY)(?![a-zA-Z])[/]?[\\S]*"
foundURLS2 <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
#Turn single @ into "at":
regex <- "@\\s"
foundAT <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"at ")
#Remove user names (@username):
regex <- "@[\\S]+"
foundUSER <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
regex <- "\\.([a-zA-Z]{2,})"
foundDOTERR <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"\\. $1")
#Turn more than 2 consecutive dots into a full stop:
regex <- "\\.[\\.]{1,}"
foundEXDOTS <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"\\. ")
#Turn ellipsis dots character into a space:
regex <- "\u2026"
foundELLIPSIS <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
#Remove dot after abbreviations (like Dr., Mr., St., a.m., U.S., etc.):
regex <- "(?<!\\w|')([a-zA-Z])\\."
foundABBDOT <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"$1")
#Turn colon into a full stop:
regex <- ":"
foundCOLON <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"\\. ")
regex <- "[0-9]+"
foundNUMBERS <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"")
tidyText <- unnest_tokens(tidyText,text,text,token="sentences",to_lower = TRUE)
#Load contractions dictionary:
data(contractions)
contractions <- transmute(contractions,contraction=tolower(contraction),expanded=tolower(expanded))
contractions <- rbind(contractions,c("y'all","you all"))
contractions <- rbind(contractions,c("here's","here is"))
contractions <- rbind(contractions,c("haven't","have not"))
#Turn curly left/right single/double quotation marks and acute accent characters into regular ones:
regex <- "\u2018|\u2019|\u00B4"
foundQUOTE <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"'")
regex <- "\u201C|\u201D"
foundDQUOTE <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex,"\"")
#Expand contraction using the dictionary:
foundCONT <- 0
for(i in seq(1,length(contractions$contraction)))
{
foundCONT <- foundCONT + sum(stri_count_fixed(tidyText$text,contractions$contraction[i]))
tidyText$text <- stri_replace_all_fixed(tidyText$text,contractions$contraction[i],contractions$expanded[i])
}
#Remove possessive ('s):
regex <- "'s\\s"
foundPOSS <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
regex <- "[[:punct:]\\^\\$]"
foundPUNCT1 <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
regex <- "\\u00F8"
foundPUNCT2 <- sum(stri_count_regex(tidyText$text,regex))
tidyText$text <- stri_replace_all_regex(tidyText$text,regex," ")
df <- data_frame(Step=c("1","1","1","1","2","3","3","3","3","4","5","6","6","7"),Description=c("e-mails","URLs","single @s","user names","word1.word2 err.","consecutive dots","ellipsis char.","abbreviations","colon","numbers","tokenization (sentences)","contraction expansions","possessives","punctuation"),Number_of_matches=c(foundEMAILS,foundURLS1+foundURLS2,foundAT,foundUSER,foundDOTERR,foundEXDOTS,foundELLIPSIS,foundABBDOT,foundCOLON,foundNUMBERS,length(tidyText$text),foundCONT,foundPOSS,foundPUNCT1+foundPUNCT2))
kable(df) %>%
kable_styling(bootstrap_options = c("striped", "hover","condensed"),full_width = FALSE)
tidyTextWords <- unnest_tokens(tidyText,words,text,token="words")
wordsCount.all <- tidyTextWords %>%
count(words,sort=TRUE)
wordsCount.blogs <- tidyTextWords %>%
filter(fileName=="blogs") %>%
count(words,sort=TRUE)
wordsCount.news <- tidyTextWords %>%
filter(fileName=="news") %>%
count(words,sort=TRUE)
wordsCount.twitter <- tidyTextWords %>%
filter(fileName=="twitter") %>%
count(words,sort=TRUE)
wordsTotal.all <- sum(wordsCount.all$n)
wordsTotal.blogs <- sum(wordsCount.blogs$n)
wordsTotal.news <- sum(wordsCount.news$n)
wordsTotal.twitter <- sum(wordsCount.twitter$n)
df <- cbind.data.frame(wordsCount.all[1:20,],wordsCount.blogs[1:20,],wordsCount.news[1:20,],wordsCount.twitter[1:20,])
kable(df) %>%
kable_styling(bootstrap_options = c("striped", "hover","condensed"),full_width = FALSE) %>%
add_header_above(c("All" = 2,"Blogs" = 2,"News" = 2,"Twitter" = 2))
df <- data_frame(All=sort(wordsCount.all$words[1:20]),Blogs=sort(wordsCount.blogs$words[1:20]),News=sort(wordsCount.news$words[1:20]),Twitter=sort(wordsCount.twitter$words[1:20]),COCA=sort(c("the","be","and","of","to","a","in","have","it","i","that","for","you","he","with","on","do","say","this","they")))
kable(df) %>%
kable_styling(bootstrap_options = c("striped","hover","condensed"),full_width = FALSE)
wordsCount.all[1:20,] %>%
mutate(words=reorder(words,n)) %>%
ggplot(aes(words,n)) +
geom_col(fill = "darkblue") +
xlab(NULL) +
coord_flip()
freq_by_rank.all <- wordsCount.all %>%
mutate(rank=row_number(),frequency=n/wordsTotal.all,file="All")
freq_by_rank.blogs <- wordsCount.blogs %>%
mutate(rank=row_number(),frequency=n/wordsTotal.blogs,file="Blogs")
freq_by_rank.news <- wordsCount.news %>%
mutate(rank=row_number(),frequency=n/wordsTotal.news,file="News")
freq_by_rank.twitter <- wordsCount.twitter %>%
mutate(rank=row_number(),frequency=n/wordsTotal.twitter,file="Twitter")
df <- rbind.data.frame(freq_by_rank.all,freq_by_rank.blogs,freq_by_rank.news,freq_by_rank.twitter)
#fit <- lm(log10(frequency)~log10(rank),data=subset(df,rank>10&rank<1000))
fit <- lm(log10(frequency)~log10(rank),data=subset(df,rank<1000))
df %>%
ggplot(aes(rank,frequency,color=file)) +
geom_line(size=1.2,alpha=0.8) +
geom_abline(intercept = fit$coefficients[1],slope=fit$coefficients[2],color="grey50",linetype=2,size=1) +
scale_x_log10() +
scale_y_log10()
df <- freq_by_rank.all
df$file <- NULL
df %>%
mutate(frequency_estimate=df$frequency[1]/rank) %>%
kable(df) %>%
kable_styling(bootstrap_options = c("striped","hover","condensed"),full_width = FALSE)
df <- freq_by_rank.all
df$file <- NULL
df %>%
mutate(frequency_estimate=df$frequency[1]/rank)
kable(df) %>%
kable_styling(bootstrap_options = c("striped","hover","condensed"),full_width = FALSE)
df <- freq_by_rank.all
df$file <- NULL
df %>%
mutate(frequency_estimate=df$frequency[1]/rank) %>%
ggplot(aes(rank)) +
geom_histogram() +
df <- freq_by_rank.all
df$file <- NULL
df %>%
mutate(frequency_estimate=df$frequency[1]/rank) %>%
ggplot() +
geom_histogram(aes(rank)) +
df <- freq_by_rank.all
df$file <- NULL
df[,1:100] %>%
mutate(frequency_estimate=df$frequency[1]/rank) %>%
ggplot(aes(rank)) +
geom_histogram() +
install.packages(c("BH", "clue", "CORElearn", "HistData", "httr", "ISLR", "knitr", "NLP", "plotrix", "quanteda", "RcppArmadillo", "rlang", "RMySQL", "rsconnect", "scales", "shiny", "tibble", "tidyr"))
install.packages("scales")
install.packages("scales")
shiny::runApp('App/word_o_matic_suggestr')
